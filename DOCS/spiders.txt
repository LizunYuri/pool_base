
Техническая документация
Название: Скрипт для обновления данных товаров
Описание:
Данный проект представляет собой паук на основе фреймворка Scrapy, который предназначен для сбора и обновления информации о товарах с определённых веб-страниц. Обновления включают в себя парсинг данных о товарах, проверку цен, их обновление и сохранение. Результаты работы сохраняются в локальные файлы и отправляются на сервер через API.

Компоненты:
Импортируемые модули:

re: для регулярных выражений, используемых при поиске и извлечении цен.
scrapy: основной фреймворк для работы с пауками.
json: для работы с JSON-данными (чтение и запись).
datetime: для работы с датой и временем (добавление временных меток).
requests: для отправки HTTP-запросов (POST-запросов на сервер).
os: для работы с файловой системой (создание директорий, запись файлов).
HttpError: для обработки ошибок HTTP-запросов.
Глобальные переменные:

base_url: URL, который указывает на локальный сервер (например, http://localhost:8000/), откуда берутся данные и куда отправляются результаты.
Классы:
1. BaseUpdateSpider (Класс)
Это базовый класс для всех пауков, которые обновляют данные о товарах. Он содержит основную логику для сбора данных, их обработки и отправки на сервер.

Атрибуты:

endpoint: указывается при инициализации паука и служит для формирования конечного URL для обновления товаров.
products: список для хранения всех товаров.
messages: список для хранения сообщений об ошибках или успешных действиях, которые отправляются в Django-сервер.
Методы:

parse: основной метод для парсинга данных с указанного URL. Формирует структуру товара и может отправлять запросы на страницы с более детальной информацией о каждом товаре.
create_product_info: создает словарь с информацией о каждом товаре (id, имя, url, данные поставщика, цена, дата).
handle_missing_product_url: обрабатывает случаи, когда у товара отсутствует URL. Генерирует сообщение об ошибке.
parse_product_page: парсит страницу товара для поиска цены, используя XPath и регулярные выражения. Если цена найдена, она обновляется.
handle_error: обрабатывает ошибки запросов, такие как 404, и сохраняет их в список сообщений.
update_price: обновляет цену товара и добавляет соответствующее сообщение в список.
close: метод, вызываемый при завершении работы паука. Сохраняет все сообщения и товары в файлы, отправляет данные на сервер.
send_logistic_messages: отправляет все сообщения на сервер через API /logistic.
save_messages_to_json: сохраняет все сообщения в JSON файл.
save_to_file: сохраняет данные о товарах в JSON файл и отправляет их на сервер.
2. UpdateFilters (Класс)
Этот класс наследует функциональность от BaseUpdateSpider. Он предназначен для обновления данных о фильтрах, получая данные с эндпоинта /update/export-filters/.

Атрибуты:
name: название паука, используемое при его запуске (например, update_filters).
start_urls: список URL, с которых начинается сбор данных (в данном случае это http://localhost:8000/update/export-filters/).
3. UpdatePumps (Класс)
Этот класс также наследует BaseUpdateSpider и предназначен для обновления данных о насосах с эндпоинта /update/export-pumps/.

Атрибуты:
name: название паука для насосов (update_pumps).
start_urls: URL для парсинга данных о насосах (http://localhost:8000/update/export-pumps/).
4. UpdateFinished (Класс)
Этот класс используется для обновления данных о готовых товарах и работает с эндпоинтом /update/export-finished/.

Атрибуты:
name: название паука (update_finished).
start_urls: URL для готовых товаров (http://localhost:8000/update/export-finished/).
Основные этапы работы пауков:
Парсинг данных:

Пауки получают данные с заданных URL-адресов (например, фильтры, насосы, готовые товары).
Если у товара есть URL, паук отправляет дополнительный запрос для парсинга его страницы и обновления цены.
Если цена товара не найдена, паук сохраняет сообщение с предупреждением.
Обработка ошибок:

Пауки могут обрабатывать различные HTTP-ошибки (например, 404). При ошибках создаются соответствующие сообщения.
Сохранение данных:

Все товары и сообщения сохраняются в JSON файлы для дальнейшего использования.
Пауки отправляют данные о товарах и сообщения на сервер через HTTP POST-запросы.
Примеры использования:
Для запуска паука для обновления фильтров можно использовать следующую команду:

bash

scrapy crawl update_filters
Для обновления данных о насосах:

bash

scrapy crawl update_pumps
Для обновления готовых товаров:

bash

scrapy crawl update_finished
Логирование:
Логи пауков хранят информацию о ходе выполнения, успешных обновлениях, а также обо всех ошибках и исключениях, что помогает в отладке и мониторинге.

Заключение:
Этот код реализует эффективную систему для сбора, обработки и обновления данных о товарах с веб-сайтов и API. Пауки используют Scrapy для автоматического парсинга и обработки информации, а также поддерживают интеграцию с сервером для обновления базы данных и отправки логистических сообщений.